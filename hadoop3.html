<!DOCTYPE html>
<html>
<head>
    <title>Week 3: Hadoop MapReduce</title>
</head>
<body>
    <h2>Introduction to MapReduce Programming Model</h2>
    <p>MapReduce is a programming model and processing framework used in Hadoop for distributed data processing. It simplifies the processing of large datasets by breaking tasks into smaller, parallelizable tasks. The MapReduce model consists of two main phases: the Map phase and the Reduce phase.</p>
    <ol>
        <li><strong>Map Phase:</strong>
            <ul>
                <li>Input data is divided into chunks called input splits.</li>
                <li>A user-defined Map function processes each input split and produces a set of intermediate key-value pairs.</li>
                <li>The Map function is applied in parallel to multiple input splits, enabling efficient data processing.</li>
            </ul>
        </li>
        <li><strong>Shuffle and Sort:</strong>
            <ul>
                <li>The intermediate key-value pairs generated by Map functions are shuffled and sorted by key across the cluster.</li>
                <li>This ensures that all values associated with a particular key are grouped together for the Reduce phase.</li>
            </ul>
        </li>
        <li><strong>Reduce Phase:</strong>
            <ul>
                <li>A user-defined Reduce function takes the sorted intermediate key-value pairs as input and processes them.</li>
                <li>The Reduce function produces the final output by aggregating or summarizing the values associated with each key.</li>
                <li>Like the Map phase, the Reduce phase can run in parallel across multiple nodes in the cluster.</li>
            </ul>
        </li>
    </ol>
    <h3>Writing Your First MapReduce Program</h3>
    <p>Let's create a simple MapReduce program in Java to count the occurrences of words in a text file. This is a classic example to illustrate the MapReduce model.</p>
    <pre>
        <code>
            <!-- Insert your Java code here -->
        </code>
    </pre>
    <p>In this code, we define a MapReduce job to count words in a text file. The <code>TokenizerMapper</code> class reads input text, tokenizes it into words, and emits key-value pairs where the word is the key and the count is one. The <code>IntSumReducer</code> class then aggregates the counts for each word and produces the final word count.</p>
    <h3>MapReduce Workflow in Hadoop</h3>
    <p>The MapReduce workflow in Hadoop involves several steps:</p>
    <ol>
        <li><strong>Data Input:</strong>
            <ul>
                <li>Input data is stored in HDFS or another Hadoop-compatible distributed file system.</li>
                <li>The user specifies the input path(s) for the MapReduce job.</li>
            </ul>
        </li>
        <li><strong>Map Phase:</strong>
            <ul>
                <li>The MapReduce job is submitted to the Hadoop cluster.</li>
                <li>Hadoop's ResourceManager schedules tasks for the Map phase on available cluster nodes.</li>
                <li>Map tasks read input data, apply the user-defined Map function, and produce intermediate key-value pairs.</li>
            </ul>
        </li>
        <li><strong>Shuffle and Sort:</strong>
            <ul>
                <li>Intermediate key-value pairs generated by Map tasks are shuffled and sorted by key.</li>
                <li>This step ensures that all values for a given key are grouped together for the Reduce phase.</li>
            </ul>
        </li>
        <li><strong>Reduce Phase:</strong>
            <ul>
                <li>Hadoop's ResourceManager schedules Reduce tasks.</li>
                <li>Reduce tasks read the sorted intermediate data, apply the user-defined Reduce function, and produce the final output key-value pairs.</li>
            </ul>
        </li>
        <li><strong>Output:</strong>
            <ul>
                <li>The final output key-value pairs are typically stored in HDFS or another storage system.</li>
                <li>Users can access and analyze the output data as needed.</li>
            </ul>
        </li>
        <li><strong>Job Monitoring:</strong>
            <ul>
                <li>Hadoop provides monitoring and logging facilities to track job progress, resource usage, and errors.</li>
            </ul>
        </li>
    </ol>
    <p>This workflow demonstrates how Hadoop MapReduce efficiently processes large datasets by dividing tasks into smaller, parallelizable units and leveraging the distributed nature of the Hadoop cluster.</p>
</body>
</html>