<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 1: Introduction to Deep Learning</title>
</head>
<body>
    <h1>Lecture 1: What is Deep Learning?</h1>
    <p>In this lecture, we introduce the concept of deep learning. Deep learning is a subfield of machine learning that focuses on training artificial neural networks to perform tasks such as image recognition, natural language processing, and more. We'll discuss the basic ideas behind deep learning, its applications, and how it differs from traditional machine learning.</p>
    <h1>Lecture 2: History and Evolution of Deep Learning</h1>
    <p>In this lecture, we delve into the history and evolution of deep learning. We'll explore key milestones, breakthroughs, and the development of deep neural networks. Understanding the historical context is essential for appreciating the rapid progress in this field.</p>
    <h1>Lecture 3: Neural Networks and Neurons</h1>
    <p>In this lecture, we dive into the fundamental building blocks of deep learning: neurons and neural networks. We'll explain how artificial neurons are inspired by their biological counterparts and how they process information. We'll also discuss activation functions and how they introduce non-linearity into neural networks.</p>
    <pre>
        <code class="python">
# Example of a simple artificial neuron with a step function activation
import numpy as np
# Define the input values
inputs = np.array([0.5, 0.7, 0.2])
# Define the weights for each input
weights = np.array([0.3, 0.5, 0.1])
# Compute the weighted sum of inputs
weighted_sum = np.dot(inputs, weights)
# Define a step function as the activation function
def step_function(x):
    if x >= 0.5:
        return 1
    else:
        return 0
# Apply the activation function to the weighted sum
output = step_function(weighted_sum)
print("Output:", output)
        </code>
    </pre>
    <h1>Lecture 4: Feedforward Neural Networks</h1>
    <p>In this lecture, we introduce feedforward neural networks (also known as multilayer perceptrons). These networks consist of multiple layers of neurons, including an input layer, one or more hidden layers, and an output layer. We'll explain how information flows through these networks, including the process of forward propagation.</p>
    <pre>
        <code class="python">
# Example of a simple feedforward neural network
import numpy as np
# Define the input values
inputs = np.array([0.5, 0.7, 0.2])
# Define the weights for each input in the first hidden layer
weights_input_hidden = np.array([[0.3, 0.2, 0.1],
                                 [0.5, 0.4, 0.6]])
# Define the weights for each neuron in the output layer
weights_hidden_output = np.array([0.7, 0.9])
# Compute the weighted sum of inputs for the hidden layer
hidden_layer_input = np.dot(weights_input_hidden, inputs)
# Define a sigmoid activation function for the hidden layer
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
# Apply the activation function to the hidden layer input
hidden_layer_output = sigmoid(hidden_layer_input)
# Compute the weighted sum of inputs for the output layer
output_layer_input = np.dot(weights_hidden_output, hidden_layer_output)
# Apply a sigmoid activation function to the output layer input
output = sigmoid(output_layer_input)
print("Output:", output)
        </code>
    </pre>
</body>
</html>